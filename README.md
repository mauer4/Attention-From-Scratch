# Attention-From-Scratch

[Project microsite](https://mauer4.github.io/attention-from-scratch/)

Attention-From-Scratch rebuilds the large language model inference stack from environment provisioning to custom CUDA kernels. The goal is to make open-weight models (OLMo, LLaMA, and custom engines) reproducible on local workstations or cloud GPUs while keeping every layer observable and verifiable.

**Last Verified:** See `reports/environment_report.md` (generated by `setup_env/install_all.sh`).

## Quick Start

```bash
bash setup_env/install_all.sh
python scripts/download_weights.py --model-name olmo2
python scripts/test_inference.py
```

The unified installer walks through GPU detection, virtual environment creation, dependency locking, configuration validation, sanity inference, and report generation. After it finishes, inspect `reports/environment_report.md` for a complete status summary.

## Environment Layers

The automation follows a layered contract:

1. **Hardware & Drivers** – `setup_env/check_gpu.py` inspects `nvidia-smi`, `nvcc`, cuDNN, and NCCL.
2. **Virtual Environment** – `setup_env/create_venv.sh` provisions `.venv/` (with a conda fallback for constrained systems).
3. **Python Dependencies** – `setup_env/install_deps.sh` installs `requirements.txt`, makes the repo editable (`pip install -e .`), reconciles the correct torch wheel, and locks dependencies.
4. **Project Code** – Python packages under `src/` (`inference`, `custom_engine`) and `analysis/` stay importable and ready for experimentation.
5. **Configuration** – `setup_env/verify_config.py` cross-checks `configs/default.yaml` and `.env`, ensuring the referenced weight and tokenizer assets are in place.
6. **Sanity Tests** – `scripts/test_inference.py` loads the staged model, generates a couple of tokens, and emits `reports/test_summary.{json,md}`.
7. **Reporting** – `setup_env/run_env_report.py` compiles Markdown/HTML summaries so runs are auditable across machines.

Read more in `docs/ENVIRONMENT.md`.

## Repository Layout

- `setup_env/` – Environment automation (`check_gpu.py`, `create_venv.sh`, `install_deps.sh`, `verify_config.py`, `run_env_report.py`, `install_all.sh`).
- `src/custom_engine/` – Custom engine scaffolding and adapters.
- `src/inference/` – Inference orchestration helpers reused by open-source and bespoke engines.
- `analysis/` – Benchmark utilities (`benchmark_runtime.py`, `compare_engines.py`, `gpu_profile_report.py`, `plot_throughput.ipynb`).
- `weights/` – Model storage (`olmo2/`, `llama3/`), manifests live under `reports/`.
- `configs/` – YAML configs (`default`, `vast`) mirrored by `.env`.
- `scripts/` – Operational helpers (`run_olmo2_inference.py`, `download_weights.py`, `test_inference.py`, etc.).
- `reports/` – Generated artefacts (`system_gpu.json`, `pip_tree.txt`, manifests, test summaries, environment reports, change log).
- `docs/` – Guidance: `SETUP.md`, `ENVIRONMENT.md`, `ARCHITECTURE.md`, `TROUBLESHOOTING.md`, and `CHANGELOG.md`.
- `codex_rules.yaml` – Self-maintenance policy executed by verification automations.

Historical planning notes remain in `docs/PROJECT_PLAN.md` and related documents.

## Custom Engine Progress

The inference engine is evolving toward CUDA kernels and bespoke runtime graphs. Until then:

- Use `setup_env/install_all.sh` to keep the environment reproducible.
- Benchmark baselines with the Hugging Face runners (see `scripts/run_open_inference.py` or custom notebooks).
- Track milestones in `docs/ARCHITECTURE.md` and `docs/PROJECT_PLAN.md`.

## Troubleshooting

Common fixes for CUDA mismatches, missing weights, and Vast.ai mount permissions are documented in `docs/TROUBLESHOOTING.md`.
